{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab30adb-ca8a-4ca3-9cbc-dcae6e244754",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7513b2a1-0749-44b5-88a9-d91d5b175e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "Finish the following function for computing a fibonacci sequence: \n",
      "\n",
      "def fib(n):\n",
      "    if n<=1:\n",
      "        return n\n",
      "    else:\n",
      "        return fib(n-1)+fib(n-2)\n",
      "    \n",
      "print(fib(15))\n",
      "\n",
      "\n",
      "50\n",
      "Finish the following function for computing a fibonacci sequence: \n",
      "\n",
      "def fib(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 1) + fib(n - 2)\n",
      "\n",
      "n = int(\n",
      "55\n",
      "Finish the following function for computing a fibonacci sequence: \n",
      "\n",
      "def fib(n):\n",
      "  x = 1\n",
      "  y = 1\n",
      "  if n < 2:\n",
      "    return 1\n",
      "  else:\n",
      "    for i in range(2, n):\n",
      "      z = x + y\n",
      "      x = y\n",
      "      y = z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:5543/generate\"\n",
    "sequence = \"Finish the following function for computing a fibonacci sequence: \\n\\ndef fib(n):\"\n",
    "# sequence = \"def fib(n):\"\n",
    "\n",
    "def request_task(seed, max_new_tokens):\n",
    "    obj = {\n",
    "        \"inputs\": sequence,\n",
    "        \"generation_parameters\": {\n",
    "            \"max_new_tokens\":max_new_tokens,\n",
    "            # \"repetition_penalty\": 1.1,\n",
    "            # \"do_sample\": True,\n",
    "            # \"temperature\": 1.1,\n",
    "            # \"top_k\": 3,\n",
    "            # \"top_p\": 0.9,\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "    }\n",
    "    with requests.post(url, json=obj) as r:\n",
    "        print(max_new_tokens)\n",
    "        dct = json.loads(r.text)\n",
    "        print(f'{sequence}{dct[\"response_text\"]}')\n",
    "\n",
    "max_new_tokens_lst = [55, 50, 45]\n",
    "seeds = [1,2,3]\n",
    "# max_new_tokens_lst = [100, 200, 300]\n",
    "\n",
    "request_ts = [\n",
    "    Thread(target=request_task, args=[seed, max_new_tokens]) for seed, max_new_tokens in zip(seeds, max_new_tokens_lst)\n",
    "]\n",
    "\n",
    "import time\n",
    "for request_t in request_ts:\n",
    "    request_t.start()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "for request_t in request_ts:\n",
    "    request_t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28964c0b-0822-48aa-bc4c-a519035fae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = {\n",
    "        \"inputs\": sequence,\n",
    "        \"generation_parameters\": {\n",
    "            \"max_new_tokens\":100,\n",
    "            \"repetion_penalty\": 2.0,\n",
    "            # \"do_sample\": False,\n",
    "            # \"temperature\": 1.0,\n",
    "            # \"top_k\": 1,\n",
    "            # \"top_p\": 0.9,\n",
    "            # \"seed\": 43,\n",
    "        }\n",
    "    }\n",
    "\n",
    "resp = requests.post(url, json=obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdb67325-1341-4da4-808f-c1b12fdbf607",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\n",
    "token = '\\n'\n",
    "for i in range(5):\n",
    "    a += token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96aebc29-2c43-456e-ab62-3c422feb57c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85447c0d-7020-46af-826f-fec58c9097b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib(n-1) + fib(n-2)\\n\\ndef fib2(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib2(n-1) + fib2(n-2)\\n\\ndef\"\n"
     ]
    }
   ],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff94c668-5832-4096-a723-3fc673b90495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robertgshaw/deepsparse-continuous-batching/deepsparse\n"
     ]
    }
   ],
   "source": [
    "%cd deepsparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d314c9f2-3fa7-44c4-8609-3bb24d008bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  1.  4.  3.  2.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from logits_process import TopKLogitsWarper, TopPLogitsWarper, softmax\n",
    "\n",
    "scores = np.array([[-1.,1.,4.,3.,2.,1.]])\n",
    "\n",
    "sorted_indices = np.argsort(scores, axis=-1)\n",
    "sorted_scores = np.take_along_axis(scores, sorted_indices, axis=-1)\n",
    "\n",
    "# sort, grabbing all those outside top_p\n",
    "cumulative_probs = softmax(sorted_scores, axis=-1).cumsum(axis=-1)\n",
    "sorted_indices_to_remove = cumulative_probs <= (1 - 0.9)\n",
    "\n",
    "\n",
    "# note: this relies on b=1\n",
    "indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "\n",
    "# set removed indices logits to -Inf (never selected)\n",
    "scores[:, indices_to_remove] = -float(\"Inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cced6917-8017-405a-ae93-c43105fce82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.  1.  5.  3.  2.  1.]\n",
      " [ 5. -2.  3. -1.  2.  1.]]\n",
      "[[-inf -inf   5.   3.   2. -inf]\n",
      " [  5. -inf   3. -inf   2. -inf]]\n",
      "[[0.         0.         0.84379473 0.1141952  0.04201007 0.        ]\n",
      " [0.84379473 0.         0.1141952  0.         0.04201007 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from logits_process import TopKLogitsWarper, TopPLogitsWarper, softmax\n",
    "\n",
    "logits = np.array([[-2.,1.,5.,3.,2.,1.], [5.,-2.,3.,-1.,2.,1.]])\n",
    "print(logits)\n",
    "\n",
    "top_k = TopKLogitsWarper(top_k=3)\n",
    "new_logits = top_k(logits)\n",
    "print(new_logits)\n",
    "\n",
    "print(softmax(new_logits, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "44fb3e44-0888-49af-a5d6-2c908db324ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  1.  4.  3.  2.  1.]]\n",
      "[[0.00418629 0.03093274 0.62130062 0.22856372 0.0840839  0.03093274]]\n",
      "[[-inf -inf   4.   3.   2. -inf]]\n",
      "[[0.         0.         0.66524096 0.24472847 0.09003057 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "logits = np.array([[-1.,1.,4.,3.,2.,1.]])\n",
    "print(logits)\n",
    "\n",
    "print(softmax(logits, axis=-1))\n",
    "\n",
    "top_p = TopPLogitsWarper(top_p=0.9)\n",
    "new_logits = top_p(logits)\n",
    "print(new_logits)\n",
    "\n",
    "print(softmax(new_logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f20813aa-0362-4820-a3b2-c32531658718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.   1.   3.   3.5  2.5  2.   1. ]]\n",
      "[[0.00468177 0.03459387 0.25561604 0.4214396  0.15503896 0.09403589\n",
      "  0.03459387]]\n",
      "[[-inf -inf  3.   3.5  2.5  2.  -inf]]\n",
      "[[-inf -inf  3.   3.5  2.5 -inf -inf]]\n",
      "[[0.         0.         0.30719589 0.50648039 0.18632372 0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "logits = np.array([[-1.,1.,3.,3.5,2.5,2.,1.]])\n",
    "print(logits)\n",
    "\n",
    "print(softmax(logits, axis=-1))\n",
    "\n",
    "top_p = TopPLogitsWarper(top_p=0.9)\n",
    "top_k = TopKLogitsWarper(top_k=3)\n",
    "\n",
    "new_logits = top_p(logits)\n",
    "print(new_logits)\n",
    "\n",
    "new_new_logits = top_k(new_logits)\n",
    "print(new_new_logits)\n",
    "print(softmax(logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6914e436-350c-4c6a-b5ca-4146f349b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.   1.   3.   5.   2.5  2.   1. ]]\n",
      "[[0.00189751 0.01402082 0.10360061 0.76551075 0.06283695 0.03811254\n",
      "  0.01402082]]\n",
      "[[-inf  1.   3.   5.   2.5  2.   1. ]]\n",
      "[[-inf -inf  3.   5.   2.5 -inf -inf]]\n"
     ]
    }
   ],
   "source": [
    "logits = np.array([[-1.,1.,3.,5,2.5,2.,1.]])\n",
    "print(logits)\n",
    "print(softmax(logits, axis=-1))\n",
    "\n",
    "top_p = TopPLogitsWarper(top_p=0.99)\n",
    "top_k = TopKLogitsWarper(top_k=3)\n",
    "\n",
    "new_logits = top_p(logits)\n",
    "print(new_logits)\n",
    "\n",
    "new_new_logits = top_k(new_logits)\n",
    "print(new_new_logits)\n",
    "# print(softmax(new_new_logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ccfb53-8e40-46b4-9987-3081add2ccdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_probs <= (1 - top_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483cea72-9955-4742-b599-6f7ec7a6c6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4, 3, 1, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38abf685-b4ae-4dc3-ab45-acf6dfd4036f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NextTokenChooser\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# picks largest logits\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ntc \u001b[38;5;241m=\u001b[39m NextTokenChooser(do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import NextTokenChooser\n",
    "\n",
    "# picks largest logits\n",
    "ntc = NextTokenChooser(do_sample=False)\n",
    "logits = np.array([[-2,1,3,2]])\n",
    "input_ids = np.array([[1,2,3,4]])\n",
    "ntc(input_ids, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a599ccae-6457-4bb9-8240-703aaf64474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0040953  0.08225629 0.60779634 0.22359578 0.08225629]]\n",
      "sample / actual 0:  0.004 /  0.004\n",
      "sample / actual 1:  0.081 /  0.082\n",
      "sample / actual 2:  0.615 /  0.608\n",
      "sample / actual 3:  0.219 /  0.224\n",
      "sample / actual 4:  0.081 /  0.082\n"
     ]
    }
   ],
   "source": [
    "# samples\n",
    "ntc = NextTokenChooser(do_sample=True)\n",
    "logits = np.array([[-2,1,3,2,1]])\n",
    "input_ids = np.array([[1,2,3,4]])\n",
    "\n",
    "probs = ntc.choice.softmax(logits)\n",
    "print(probs)\n",
    "\n",
    "iters = 10000\n",
    "counts = {a: 0 for a in range(logits.shape[1])}\n",
    "\n",
    "for _ in range(iters):\n",
    "    pred = ntc(input_ids, logits)\n",
    "    counts[pred] += 1\n",
    "\n",
    "for i in range(logits.shape[1]):\n",
    "    print(f\"sample / actual {i}: {counts[i] / iters: 0.3f} / {probs[0,i]: 0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76f4d55d-7314-46e8-91e8-4a237d9d2ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0040953  0.08225629 0.60779634 0.22359578 0.08225629]]\n",
      "sample / actual 0:  0.066 /  0.004\n",
      "sample / actual 1:  0.176 /  0.082\n",
      "sample / actual 2:  0.342 /  0.608\n",
      "sample / actual 3:  0.244 /  0.224\n",
      "sample / actual 4:  0.172 /  0.082\n"
     ]
    }
   ],
   "source": [
    "# should pick logits that are less likely\n",
    "ntc = NextTokenChooser(do_sample=True, temperature=3.0)\n",
    "logits = np.array([[-2,1,3,2,1]])\n",
    "input_ids = np.array([[1,2,3,4]])\n",
    "\n",
    "probs = ntc.choice.softmax(logits)\n",
    "print(probs)\n",
    "\n",
    "iters = 10000\n",
    "counts = {a: 0 for a in range(logits.shape[1])}\n",
    "\n",
    "for _ in range(iters):\n",
    "    pred = ntc(input_ids, logits)\n",
    "    counts[pred] += 1\n",
    "\n",
    "for i in range(logits.shape[1]):\n",
    "    print(f\"sample / actual {i}: {counts[i] / iters: 0.3f} / {probs[0,i]: 0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e172586d-e0f9-4429-9b73-52dccb2117ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0040953  0.08225629 0.60779634 0.22359578 0.08225629]]\n",
      "sample / actual 0:  0.197 /  0.004\n",
      "sample / actual 1:  0.201 /  0.082\n",
      "sample / actual 2:  0.205 /  0.608\n",
      "sample / actual 3:  0.203 /  0.224\n",
      "sample / actual 4:  0.195 /  0.082\n"
     ]
    }
   ],
   "source": [
    "# should approach uniform\n",
    "ntc = NextTokenChooser(do_sample=True, temperature=100.0)\n",
    "logits = np.array([[-2,1,3,2,1]])\n",
    "input_ids = np.array([[1,2,3,4]])\n",
    "\n",
    "probs = ntc.choice.softmax(logits)\n",
    "print(probs)\n",
    "\n",
    "iters = 10000\n",
    "counts = {a: 0 for a in range(logits.shape[1])}\n",
    "\n",
    "for _ in range(iters):\n",
    "    pred = ntc(input_ids, logits)\n",
    "    counts[pred] += 1\n",
    "\n",
    "for i in range(logits.shape[1]):\n",
    "    print(f\"sample / actual {i}: {counts[i] / iters: 0.3f} / {probs[0,i]: 0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "606f2be7-4ae3-4898-8c52-ec71207a5ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0040953  0.08225629 0.60779634 0.22359578 0.08225629]]\n",
      "sample / actual 0:  0.000 /  0.004\n",
      "sample / actual 1:  0.015 /  0.082\n",
      "sample / actual 2:  0.857 /  0.608\n",
      "sample / actual 3:  0.111 /  0.224\n",
      "sample / actual 4:  0.017 /  0.082\n"
     ]
    }
   ],
   "source": [
    "# should pick logits that are more likely\n",
    "ntc = NextTokenChooser(do_sample=True, temperature=0.5)\n",
    "logits = np.array([[-2,1,3,2,1]])\n",
    "input_ids = np.array([[1,2,3,4]])\n",
    "\n",
    "probs = ntc.choice.softmax(logits)\n",
    "print(probs)\n",
    "\n",
    "iters = 10000\n",
    "counts = {a: 0 for a in range(logits.shape[1])}\n",
    "\n",
    "for _ in range(iters):\n",
    "    pred = ntc(input_ids, logits)\n",
    "    counts[pred] += 1\n",
    "\n",
    "for i in range(logits.shape[1]):\n",
    "    print(f\"sample / actual {i}: {counts[i] / iters: 0.3f} / {probs[0,i]: 0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ea0c96a5-fb8f-4f21-a30e-7443d17a440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0040953  0.08225629 0.60779634 0.22359578 0.08225629]]\n",
      "sample / actual 0:  0.000 /  0.004\n",
      "sample / actual 1:  0.000 /  0.082\n",
      "sample / actual 2:  1.000 /  0.608\n",
      "sample / actual 3:  0.000 /  0.224\n",
      "sample / actual 4:  0.000 /  0.082\n"
     ]
    }
   ],
   "source": [
    "# should approximate greedy sampling\n",
    "ntc = NextTokenChooser(do_sample=True, temperature=0.001)\n",
    "logits = np.array([[-2,1,3,2,1]])\n",
    "input_ids = np.array([[1,2,3,4]])\n",
    "\n",
    "probs = ntc.choice.softmax(logits)\n",
    "print(probs)\n",
    "\n",
    "iters = 10000\n",
    "counts = {a: 0 for a in range(logits.shape[1])}\n",
    "\n",
    "for _ in range(iters):\n",
    "    pred = ntc(input_ids, logits)\n",
    "    counts[pred] += 1\n",
    "\n",
    "for i in range(logits.shape[1]):\n",
    "    print(f\"sample / actual {i}: {counts[i] / iters: 0.3f} / {probs[0,i]: 0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c9dcebaa-fa27-4d32-ad95-6a566d808868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "3\n",
      "1\n",
      "sample / actual / original 0:  0.017 /  0.016 /  0.004\n",
      "sample / actual / original 1:  0.119 /  0.122 /  0.032\n",
      "sample / actual / original 2:  0.204 /  0.201 /  0.236\n",
      "sample / actual / original 3:  0.329 /  0.331 /  0.087\n",
      "sample / actual / original 4:  0.330 /  0.331 /  0.641\n"
     ]
    }
   ],
   "source": [
    "# should approximate greedy sampling\n",
    "ntc = NextTokenChooser(do_sample=False)\n",
    "\n",
    "# should select index 4\n",
    "logits = np.array([[-2.,1.,3.,2.,5.]])\n",
    "input_ids = np.array([[3,4]])\n",
    "\n",
    "print(ntc(input_ids,logits))\n",
    "\n",
    "# should select index 4\n",
    "ntc = NextTokenChooser(do_sample=False, repetition_penalty=1.0)\n",
    "logits = np.array([[-2,1,3,2,5]])\n",
    "input_ids = np.array([[3,4]])\n",
    "print(ntc(input_ids,logits))\n",
    "\n",
    "# should select index 2\n",
    "ntc = NextTokenChooser(do_sample=False, repetition_penalty=2.0)\n",
    "logits = np.array([[-2.,1.,3.,2.,5.]])\n",
    "input_ids = np.array([[3,4]])\n",
    "print(ntc(input_ids,logits))\n",
    "\n",
    "# should select index 4\n",
    "logits = np.array([[-2.,1.,3.,2.,5.]])\n",
    "input_ids = np.array([[2,4]])\n",
    "ntc = NextTokenChooser(do_sample=False, repetition_penalty=2.)\n",
    "print(ntc(input_ids,logits))\n",
    "\n",
    "# should select index 3\n",
    "logits = np.array([[-2.,1.,3.,2.,5.]])\n",
    "input_ids = np.array([[2,4]])\n",
    "ntc = NextTokenChooser(do_sample=False, repetition_penalty=3.)\n",
    "print(ntc(input_ids,logits))\n",
    "\n",
    "# should select index 1\n",
    "logits = np.array([[-2.,1.,3.,2.,5.]])\n",
    "input_ids = np.array([[2,3,4]])\n",
    "ntc = NextTokenChooser(do_sample=False, repetition_penalty=5.)\n",
    "print(ntc(input_ids,logits))\n",
    "\n",
    "\n",
    "# should make 2,4 less liekly\n",
    "logits_og = np.array([[-1.,1.,3.,2.,4.]])\n",
    "input_ids = np.array([[2,4]])\n",
    "ntc = NextTokenChooser(do_sample=True, repetition_penalty=2.)\n",
    "\n",
    "original_ps = ntc.choice.softmax(logits_og,axis=1)\n",
    "penalty = np.array([[1.,1.,2.,1.,2.]])\n",
    "penalty_ps = ntc.choice.softmax(logits_og/penalty,axis=1)\n",
    "\n",
    "iters = 20000\n",
    "counts = {a: 0 for a in range(logits.shape[1])}\n",
    "\n",
    "for _ in range(iters):\n",
    "    logits = logits_og.copy()\n",
    "    # print(logits)\n",
    "    pred = ntc(input_ids, logits)\n",
    "    counts[pred] += 1\n",
    "\n",
    "for i in range(logits.shape[1]):\n",
    "    print(f\"sample / actual / original {i}: {counts[i] / iters: 0.3f} / {penalty_ps[0,i]: 0.3f} / {original_ps[0,i]: 0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "afd3139e-924c-4ac0-a450-c462447a610a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2. ,  1. ,  1.5,  2. ,  2.5]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits / penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea9f993e-12f8-4bd0-9f8c-885bf8e56a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00446236 0.08962882 0.24363641 0.66227241]]\n",
      "{0: 0.0041, 1: 0.0909, 2: 0.2418, 3: 0.6632}\n"
     ]
    }
   ],
   "source": [
    "from utils import Sampling\n",
    "import numpy as np\n",
    "\n",
    "sampling = Sampling(seed=1)\n",
    "\n",
    "logits = np.array([[-2,1,2,3]])\n",
    "probs = sampling.softmax(logits, axis=1)\n",
    "\n",
    "print(probs)\n",
    "counts = {a: 0 for a in range(logits.shape[1])}\n",
    "iters = 10000\n",
    "for i in range(iters):\n",
    "    counts[sampling(logits=logits)] +=1\n",
    "\n",
    "for key in counts:\n",
    "    counts[key] /= iters\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8cd5290-a55b-44c0-ab2c-b34299a1da7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01 -0.01  0.01 -0.01  0.01 -0.01  0.01 -0.01]]\n",
      "[[0 2 5 7]]\n",
      "[[ 0.005 -0.01   0.005 -0.01   0.01  -0.02   0.01  -0.02 ]]\n",
      "[[ 0.0025 -0.005   0.0025 -0.005   0.005  -0.01    0.005  -0.01  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RepetitionPenaltyLogitsProcessor:\n",
    "    def __init__(self, penalty: float):\n",
    "        if not isinstance(penalty, float) or not (penalty > 0):\n",
    "            raise ValueError(f\"`penalty` has to be a strictly positive float, but is {penalty}\")\n",
    "\n",
    "        self.penalty = penalty\n",
    "\n",
    "    def __call__(self, scores: np.ndarray, input_ids: np.ndarray) -> np.ndarray:\n",
    "        # assert shape is [1, vocab_size]\n",
    "        assert len(scores.shape) == 2\n",
    "        assert scores.shape[0] == 1\n",
    "\n",
    "        # assert shape is [1, seq_len]\n",
    "        assert len(input_ids.shape) == 2\n",
    "        assert input_ids.shape[0] == 1\n",
    "        \n",
    "        # TODO: update logic to handle b > 1\n",
    "        score = scores[:, input_ids[0]]\n",
    "        score = np.where(score < 0, score * self.penalty, score / self.penalty)\n",
    "        scores[:, input_ids[0]] = score\n",
    "\n",
    "        return scores\n",
    "\n",
    "class TemperatureLogitsWarper:\n",
    "    def __init__(self, temperature: float):\n",
    "        if not isinstance(temperature, float) or not (temperature > 0):\n",
    "            except_msg = (\n",
    "                f\"`temperature` (={temperature}) has to be a strictly positive float, otherwise your next token \"\n",
    "                \"scores will be invalid.\"\n",
    "            )\n",
    "            if isinstance(temperature, float) and temperature == 0.0:\n",
    "                except_msg += \" If you're looking for greedy decoding strategies, set `do_sample=False`.\"\n",
    "            raise ValueError(except_msg)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, scores: np.ndarray) -> np.ndarray:\n",
    "        # assert shape is [1, vocab_size]\n",
    "        assert len(scores.shape) == 2\n",
    "        assert scores.shape[0] == 1\n",
    "\n",
    "        return scores / self.temperature\n",
    "\n",
    "input_ids = np.array([[0,2,5,7]])\n",
    "logits = np.array([[0.01, -0.01]*4])\n",
    "\n",
    "print(logits)\n",
    "print(input_ids)\n",
    "\n",
    "processor = RepetitionPenaltyLogitsProcessor(penalty=2.0)\n",
    "logits = processor(scores=logits, input_ids=input_ids)\n",
    "print(logits)\n",
    "\n",
    "warper = TemperatureLogitsWarper(temperature=2.0)\n",
    "logits = warper(scores=logits)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e548af40-7b71-4f96-98b5-f33f03ef3f66",
   "metadata": {},
   "source": [
    "p# **Interacting with FastAPI Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ec49282-dafd-4d4b-af7a-a68af7fb0dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Finish the following function for computing a fibonacci sequence: \\n\\ndef fib(n):\\n    if n == 0:\\n        \""
     ]
    }
   ],
   "source": [
    "!curl 127.0.0.1:5543/generate \\\n",
    "    -X POST \\\n",
    "    -d '{\"inputs\":\"Finish the following function for computing a fibonacci sequence: \\n\\ndef fib(n):\",\"generation_parameters\":{\"max_new_tokens\":10}}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ea583b1-e2d3-4f35-b87f-630d097a2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from threading import Thread\n",
    "\n",
    "url = \"http://127.0.0.1:5543/generate\"\n",
    "# sequence = \"Finish the following function for computing a fibonacci sequence: \\n\\ndef fib(n):\"\n",
    "sequence = \"def fib(n):\"\n",
    "\n",
    "def request_task(max_new_tokens):\n",
    "    obj = {\n",
    "        \"inputs\":sequence,\n",
    "        \"generation_parameters\": {\n",
    "            \"max_new_tokens\":max_new_tokens\n",
    "        }\n",
    "    }\n",
    "    with requests.post(url, json=obj) as r:\n",
    "        print(max_new_tokens)\n",
    "        print(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dec4413-afea-444a-90b7-c98b450d5fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "\"def fib(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib(n-1) + fib(n-2)\\n\\ndef fib2(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib2(n-1) + fib2(n-2)\\n\\ndef\"\n",
      "200\n",
      "\"def fib(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib(n-1) + fib(n-2)\\n\\ndef fib2(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib2(n-1) + fib2(n-2)\\n\\ndef fib3(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1\"\n",
      "300\n",
      "\"def fib(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib(n-1) + fib(n-2)\\n\\ndef fib2(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib2(n-1) + fib2(n-2)\\n\\ndef fib3(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1\"\n"
     ]
    }
   ],
   "source": [
    "# max_new_tokens_lst = [50, 10, 100, 25, 15]\n",
    "max_new_tokens_lst = [100, 200, 300]\n",
    "\n",
    "request_ts = [\n",
    "    Thread(target=request_task, args=[max_new_tokens]) for max_new_tokens in max_new_tokens_lst\n",
    "]\n",
    "\n",
    "import time\n",
    "for request_t in request_ts:\n",
    "    request_t.start()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "for request_t in request_ts:\n",
    "    request_t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb0e1de-17e2-4ff2-aecb-22b6f607ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Driver function to test above function\n",
      "n = int(input(\"Enter the number: \"))\n",
      "print(fib(n))\n",
      "\n",
      "# This code is contributed by Nikhil Kumar Singh(nickzuck_007)\n"
     ]
    }
   ],
   "source": [
    "!curl 127.0.0.1:5543/generate_stream \\\n",
    "    -X POST \\\n",
    "    -d '{\"prompt\":\"Finish the following function for computing a fibonacci sequence: \\n\\ndef fib(n):\",\"max_generated_tokens\":100}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6948b65b-8851-40f3-bf67-d7db2cf1955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"def fib(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib(n-1) + fib(n-2)\\n\\ndef fib2(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fib2(n-1) + fib2(n-2)\\n\\ndef fib3(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1\""
     ]
    }
   ],
   "source": [
    "!curl http://127.0.0.1:5543/generate/def%20fib%28n%29%3A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19786b8-e72c-43c1-964f-45d92fd171e9",
   "metadata": {},
   "source": [
    "## Example Interacting With The Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b2c83cd-92ea-40d7-bc7e-f737b87d9b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 17:46:45 deepsparse.transformers WARNING  The neuralmagic fork of transformers may not be installed. It can be installed via `pip install nm_transformers`\n"
     ]
    }
   ],
   "source": [
    "from server.deepsparse.router import DeepSparseRouter, batching_task\n",
    "from server.deepsparse.service.service import DeepSparseService\n",
    "from server.deepsparse.service.causal_lm import DeepSparseCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78acf813-3688-483d-9148-5c0df5d6b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "2023-08-24 17:47:02 deepsparse.transformers.utils.helpers INFO     Overwriting in-place the input shapes of the transformer model at /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.6.0.20230815 COMMUNITY | (134dba40) (release) (optimized) (system=avx2, binary=avx2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 8\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 17:47:27 deepsparse.transformers.utils.helpers INFO     Overwriting in-place the input shapes of the transformer model at /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 8\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/deployment\"\n",
    "onnx_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\"\n",
    "\n",
    "model = DeepSparseCausalLM(\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    model_path=onnx_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c011f167-f150-4a46-b24d-3074e8564151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from server.deepsparse.utils import GenerateRequest\n",
    "from queue import Queue \n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "service = DeepSparseService(model=model)\n",
    "router = DeepSparseRouter(service=service)\n",
    "batching_thread = Thread(target=batching_task, args=[router])\n",
    "batching_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538ba71e-9562-4325-899b-c67a4cf74075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting request\n",
      "starting request\n",
      "Finish the following function for computing a fibonacci sequence: \n",
      "\n",
      "def fib(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Driver function to test above function\n",
      "n = int(input(\"Enter the number: \"))\n",
      "print(fib(n))\n",
      "\n",
      "# This code is contributed by Nikhil Kumar Singh(nickzuck_007)\n",
      "\n",
      "Write a function for filtering a list of integers to include only positive numbers:\n",
      "\n",
      "def filter(lst):\n",
      "    return [x for x in lst if x > 0]\n",
      "\n",
      "# Test\n",
      "print(filter([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
      "print(filter([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
      "print(filter([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
      "print(filter([1,\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Finish the following function for computing a fibonacci sequence: \\n\\ndef fib(n):\",\n",
    "    \"Write a function for filtering a list of integers to include only positive numbers:\\n\\ndef filter(lst):\",\n",
    "    \"Write a function for reversing a string:\\n\\ndef reverse_string(s):\",\n",
    "    \"Write a function for checking if a word if a palindrome:\\n\\ndef is_palindrome(word):\",\n",
    "    \"Write a function for sorting an array of integers:\\n\\ndef merge_sort(arr):\",\n",
    "]\n",
    "\n",
    "def generate_request(prompt):\n",
    "    print(\"starting request\")\n",
    "    response_stream = Queue()\n",
    "    \n",
    "    g_request = GenerateRequest(\n",
    "        prompt=prompt,\n",
    "        max_generated_tokens=100,\n",
    "        response_stream=response_stream\n",
    "    )\n",
    "\n",
    "    router.generate(g_request)\n",
    "\n",
    "    str = prompt\n",
    "    generation = response_stream.get()\n",
    "    while not generation.stopped:\n",
    "        str += generation.token\n",
    "        generation = response_stream.get()\n",
    "\n",
    "    print(str)\n",
    "\n",
    "generate_threads = [\n",
    "    Thread(target=generate_request, args=[prompt]) for prompt in prompts[:2]\n",
    "]\n",
    "\n",
    "# print(len(generate_threads))\n",
    "\n",
    "for gt in generate_threads:\n",
    "    gt.start()\n",
    "    time.sleep(1)\n",
    "\n",
    "for gt in generate_threads:\n",
    "    gt.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a1c1cc-e1d1-4915-8c3a-c1c9f2aa5147",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m service \u001b[38;5;241m=\u001b[39m DeepSparseService(model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m      2\u001b[0m router \u001b[38;5;241m=\u001b[39m DeepSparseRouter(service\u001b[38;5;241m=\u001b[39mservice)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mbatching_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dscb/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/dscb/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "service = DeepSparseService(model=model)\n",
    "router = DeepSparseRouter(service=service)\n",
    "batching_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762fbbb-9711-4bf4-aeb9-3083a1f5b1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a556e2-d626-4733-84eb-e7e40f244981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1a7dd-de34-443a-bf37-6ef039bd25c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c50cb8f-bc1b-487d-94e2-90cee1cabdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, generate_requests = router.queue.next_batch(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c048933-7e50-4f6a-b360-41a3ae7ae8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CachedBatch(batch_id=0, request_ids=[0])\n"
     ]
    }
   ],
   "source": [
    "cached_batch = router.prefill(batch, generate_requests)\n",
    "print(cached_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43363249-5256-4259-b465-b99fc4d7f42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "next_batch = router.queue.next_batch(block=False)\n",
    "print(next_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ebb411e-c9ff-471a-9cd0-77fcdb2299df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n",
      "CachedBatch(batch_id=0, request_ids=[0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    cached_batch = router.decode(batches=[cached_batch], generate_requests=generate_requests)\n",
    "    print(cached_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1fb10db6-54bb-4fcb-8662-e12a65fc0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "router.generate(g_request1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bf59798-3325-4171-9a4b-fab283c8194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_batch = router.queue.next_batch(block=False)\n",
    "if next_batch is not None:\n",
    "    new_batch, new_generate_requests = next_batch\n",
    "\n",
    "new_cached_batch = router.prefill(\n",
    "    batch=new_batch,\n",
    "    generate_requests=new_generate_requests\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e9e5654-beb0-4fba-8bf6-49824d335925",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [cached_batch]\n",
    "batches.append(new_cached_batch)\n",
    "generate_requests.update(new_generate_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "764448bb-7841-4910-ab6b-56c3c7a2cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    cached_batch = router.decode(batches=batches, generate_requests=generate_requests)\n",
    "    batches = [cached_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1536911c-8504-41b3-aa96-cc33024446f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: GenerateRequest(prompt='Finish the following function for computing a fibonacci sequence: \\n\\ndef fib(n):', max_generated_tokens=100, response_stream=<queue.Queue object at 0x7f552c1b3910>),\n",
       " 1: GenerateRequest(prompt='Write a function for reversing a string:\\n\\ndef reverse_string(s):', max_generated_tokens=100, response_stream=<queue.Queue object at 0x7f5557c30ac0>)}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1dfb9ed-8bf0-4a38-a7f6-9cd22212e663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation(request_id=1, token='\\n', token_id=198, stopped=False)\n",
      "Generation(request_id=1, token='    ', token_id=50284, stopped=False)\n",
      "Generation(request_id=1, token='return', token_id=7783, stopped=False)\n",
      "Generation(request_id=1, token=' s', token_id=264, stopped=False)\n",
      "Generation(request_id=1, token='[', token_id=58, stopped=False)\n",
      "Generation(request_id=1, token='::', token_id=3712, stopped=False)\n",
      "Generation(request_id=1, token='-', token_id=12, stopped=False)\n",
      "Generation(request_id=1, token='1', token_id=16, stopped=False)\n",
      "Generation(request_id=1, token=']', token_id=60, stopped=False)\n",
      "Generation(request_id=1, token='\\n', token_id=198, stopped=False)\n",
      "Generation(request_id=1, token='\\n', token_id=198, stopped=False)\n",
      "Generation(request_id=1, token='#', token_id=2, stopped=False)\n",
      "Generation(request_id=1, token=' Test', token_id=6208, stopped=False)\n",
      "Generation(request_id=1, token='\\n', token_id=198, stopped=False)\n",
      "Generation(request_id=1, token='print', token_id=4798, stopped=False)\n",
      "Generation(request_id=1, token='(', token_id=7, stopped=False)\n",
      "Generation(request_id=1, token='reverse', token_id=50188, stopped=False)\n",
      "Generation(request_id=1, token='_', token_id=62, stopped=False)\n",
      "Generation(request_id=1, token='string', token_id=8841, stopped=False)\n",
      "Generation(request_id=1, token='(\"', token_id=7203, stopped=False)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    generation = response_stream1.get(block=False)\n",
    "    print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ead58c-e59b-4a9c-8b35-a1a658f22a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57985b2-75f8-4460-9b77-e531a12bd12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93bac63-8924-4cf4-8683-81ce9333a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish the following function for computing a fibonacci sequence: \n",
      "\n",
      "def fib(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Driver function to test above function\n",
      "n = int(input(\"Enter the number: \"))\n",
      "print(fib(n))\n",
      "\n",
      "# This code is contributed by Nikhil Kumar Singh(nickzuck_007)\n",
      "\n",
      "\n",
      "\n",
      "Write a function for filtering a list of integers to include only positive numbers:\n",
      "\n",
      "def filter(lst):\n",
      "    return [x for x in lst if x > 0]\n",
      "\n",
      "# Test\n",
      "print(filter([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
      "print(filter([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
      "print(filter([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
      "print(filter([1,\n",
      "\n",
      "\n",
      "Write a function for checking if a word if a palindrome:\n",
      "\n",
      "def is_palindrome(word):\n",
      "    return word == word[::-1]\n",
      "\n",
      "# Test\n",
      "print(is_palindrome(\"racecar\"))\n",
      "print(is_palindrome(\"racecar\"))\n",
      "print(is_palindrome(\"racecar\"))\n",
      "print(is_palindrome(\"racecar\"))\n",
      "print(is_palindrome(\"racecar\"))\n",
      "print(is_palindrome(\"racecar\"))\n",
      "print(is_palindrome(\"racecar\"))\n",
      "print(\n",
      "\n",
      "\n",
      "Write a function for reversing a string:\n",
      "\n",
      "def reverse_string(s):\n",
      "    return s[::-1]\n",
      "\n",
      "# Test\n",
      "print(reverse_string(\"hello\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"a\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"\"))\n",
      "print(reverse_string(\"\n",
      "\n",
      "\n",
      "Write a function for sorting an array of integers:\n",
      "\n",
      "def merge_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    mid = len(arr) // 2\n",
      "    left = arr[:mid]\n",
      "    right = arr[mid:]\n",
      "    left = merge_sort(left)\n",
      "    right = merge_sort(right)\n",
      "    return merge(left, right)\n",
      "\n",
      "def merge(left, right):\n",
      "    result = []\n",
      "    while len(left) > 0 and len(right) > 0:\n",
      "        if left[0]\n",
      "\n",
      "\n",
      "stop\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "batching_thread = Thread(target=batching_task, args=[router])\n",
    "batching_thread.start()\n",
    "\n",
    "prompts = [\n",
    "    \"Finish the following function for computing a fibonacci sequence: \\n\\ndef fib(n):\",\n",
    "    \"Write a function for filtering a list of integers to include only positive numbers:\\n\\ndef filter(lst):\",\n",
    "    \"Write a function for reversing a string:\\n\\ndef reverse_string(s):\",\n",
    "    \"Write a function for checking if a word if a palindrome:\\n\\ndef is_palindrome(word):\",\n",
    "    \"Write a function for sorting an array of integers:\\n\\ndef merge_sort(arr):\",\n",
    "]\n",
    "\n",
    "def generate_task(prompt):\n",
    "    result = router.generate(prompt=prompt)\n",
    "    print(result)\n",
    "    print(\"\\n\")\n",
    "\n",
    "generate_threads = [\n",
    "    Thread(target=generate_task, args=[prompt]) for prompt in prompts\n",
    "]\n",
    "\n",
    "# print(len(generate_threads))\n",
    "\n",
    "for gt in generate_threads:\n",
    "    gt.start()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "for gt in generate_threads:\n",
    "    gt.join()\n",
    "\n",
    "\n",
    "generate(\"stop\")\n",
    "batching_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43c041-2c79-4276-9104-2f224b2f8af6",
   "metadata": {},
   "source": [
    "## Example Interacting With The Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631e94eb-cca0-438e-8936-6e8a87166d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 14:26:39 deepsparse.transformers WARNING  The neuralmagic fork of transformers may not be installed. It can be installed via `pip install nm_transformers`\n"
     ]
    }
   ],
   "source": [
    "from server.deepsparse.deepsparse_causal_lm import DeepSparseCausalLMBatch, DeepSparseCausalLM\n",
    "from server.deepsparse.deepsparse_service import DeepSparseService\n",
    "from server.deepsparse.deepsparse_requests import (\n",
    "    PrefillRequest, DecodeRequest, FilterBatchRequest, Request\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c39557-2898-443f-aae8-443ef1171123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "2023-08-22 14:26:56 deepsparse.transformers.utils.helpers INFO     Overwriting in-place the input shapes of the transformer model at /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.6.0.20230815 COMMUNITY | (134dba40) (release) (optimized) (system=avx2, binary=avx2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 8\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 14:27:21 deepsparse.transformers.utils.helpers INFO     Overwriting in-place the input shapes of the transformer model at /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 8\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/deployment\"\n",
    "onnx_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\"\n",
    "\n",
    "model = DeepSparseCausalLM(\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    model_path=onnx_path\n",
    ")\n",
    "\n",
    "service = DeepSparseService(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ce9aab-1a56-4b6f-a82b-4e91d52290b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Finish the following function for computing a fibonacci sequence: \\n\\n fib(n):\",\n",
    "    \"Write a function for filtering a list of integers to include only positive numbers:\\n\\nfilter(lst):\",\n",
    "    \"Write a function for reversing a string:\\n\\ndef reverse_string(s):\",\n",
    "    \"Write a function for checking if a word if a palindrome:\\n\\ndef is_palindrome(word):\",\n",
    "    \"Write a function for sorting an array of integers:\\n\\ndef merge_sort(arr):\",\n",
    "]\n",
    "\n",
    "def make_batch(id, prompt):\n",
    "    return Batch(\n",
    "        id=id,\n",
    "        requests=[Request(id=id, prompt=prompt)]\n",
    "    )\n",
    "\n",
    "class PrefillQueue:\n",
    "    def __init__(self, prompts):\n",
    "        self.queue = {\n",
    "            idx: PrefillRequest(batch=make_batch(id=idx, prompt=prompt))\n",
    "            for idx, prompt in enumerate(prompts)\n",
    "        }\n",
    "\n",
    "    def pop(self):\n",
    "        keys = list(self.queue.keys())\n",
    "        if len(keys) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return self.queue.pop(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2441753-fe2a-45c0-ad80-135b6207947d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m service\u001b[38;5;241m.\u001b[39mClearCache()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# prefill queue\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m prefill_queue \u001b[38;5;241m=\u001b[39m \u001b[43mPrefillQueue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# cached batches\u001b[39;00m\n\u001b[1;32m      7\u001b[0m cached_batches \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mPrefillQueue.__init__\u001b[0;34m(self, prompts)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m         idx: PrefillRequest(batch\u001b[38;5;241m=\u001b[39mmake_batch(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39midx, prompt\u001b[38;5;241m=\u001b[39mprompt))\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts)\n\u001b[1;32m     20\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 18\u001b[0m         idx: PrefillRequest(batch\u001b[38;5;241m=\u001b[39m\u001b[43mmake_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts)\n\u001b[1;32m     20\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mmake_batch\u001b[0;34m(id, prompt)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_batch\u001b[39m(\u001b[38;5;28mid\u001b[39m, prompt):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m     12\u001b[0m         requests\u001b[38;5;241m=\u001b[39m[Request(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m, prompt\u001b[38;5;241m=\u001b[39mprompt)]\n\u001b[1;32m     13\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Batch' is not defined"
     ]
    }
   ],
   "source": [
    "service.ClearCache()\n",
    "\n",
    "# prefill queue\n",
    "prefill_queue = PrefillQueue(prompts)\n",
    "\n",
    "# cached batches\n",
    "cached_batches = []\n",
    "\n",
    "# generated\n",
    "generated_text = {}\n",
    "\n",
    "def prefill(request):\n",
    "    generation, cached_batch = service.Prefill(request)\n",
    "    \n",
    "    assert request.batch.requests[0].id == generation.request_id\n",
    "    assert generation.request_id not in generated_text.keys()\n",
    "    \n",
    "    generated_text[generation.request_id] = request.batch.requests[0].prompt + generation.generated_text\n",
    "\n",
    "    return cached_batch\n",
    "\n",
    "def decode(request):\n",
    "    for cached_batch in request.batches:\n",
    "        for request_id in cached_batch.request_ids:\n",
    "            assert request_id in generated_text.keys()\n",
    "\n",
    "    generations, cached_batch = service.Decode(request)\n",
    "    if cached_batch is None:\n",
    "        print(\"All requests done!\\n\\n\")\n",
    "        return None\n",
    "    \n",
    "    active_request_ids = []\n",
    "    stopped_request_ids = []\n",
    "    \n",
    "    for generation in generations:\n",
    "        assert generation.request_id in generated_text.keys()\n",
    "\n",
    "        # if text is None, we stopped\n",
    "        if generation.generated_text is None:\n",
    "            print(f\"Request {generation.request_id} is done!\")\n",
    "            stopped_request_ids.append(generation.request_id)\n",
    "            \n",
    "        else:\n",
    "            generated_text[generation.request_id] += generation.generated_text\n",
    "            active_request_ids.append(generation.request_id)\n",
    "        \n",
    "    # if any stopped, return this\n",
    "    if len(stopped_request_ids) > 0:\n",
    "        cached_batch = service.FilterBatch(FilterBatchRequest(\n",
    "            batch_id=cached_batch.batch_id,\n",
    "            request_ids=active_request_ids,\n",
    "        ))\n",
    "        \n",
    "    return cached_batch\n",
    "\n",
    "# run a prefille\n",
    "queue_not_empty = True\n",
    "while queue_not_empty:\n",
    "    prefill_request = prefill_queue.pop()\n",
    "    if prefill_request is not None:\n",
    "        cached_batch = prefill(prefill_request)\n",
    "        cached_batches.append(cached_batch)\n",
    "    else:\n",
    "        queue_not_empty = False\n",
    "    \n",
    "    # run a few decodes\n",
    "    for _ in range(5):\n",
    "        cached_batches = [decode(DecodeRequest(cached_batches))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6bcc43-63ef-4f92-a960-74e33b86dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a few decodes\n",
    "for _ in range(100):\n",
    "    cached_batch = decode(DecodeRequest(cached_batches))\n",
    "    if cached_batch is None:\n",
    "        break\n",
    "    cached_batches = [cached_batch]\n",
    "    \n",
    "for idx, value in generated_text.items():\n",
    "    print(f\"INDEX = {idx}:\")\n",
    "    print(value)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(cached_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9198565-a7e3-4ba4-8f46-b21adc4d87ac",
   "metadata": {},
   "source": [
    "## Example DeepSparseCausalLMBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf269cd-3d85-46c4-b80c-7d3d7199756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 01:33:22 deepsparse.transformers WARNING  The neuralmagic fork of transformers may not be installed. It can be installed via `pip install nm_transformers`\n"
     ]
    }
   ],
   "source": [
    "from server.deepsparse.deepsparse_causal_lm import DeepSparseCausalLMBatch, DeepSparseCausalLM\n",
    "from server.deepsparse.deepsparse_requests import Request, Batch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/deployment\"\n",
    "onnx_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4c3d6a-d90d-46d2-943d-4d12297599eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "2023-08-22 01:33:25 deepsparse.transformers.utils.helpers INFO     Overwriting in-place the input shapes of the transformer model at /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.6.0.20230815 COMMUNITY | (134dba40) (release) (optimized) (system=avx2, binary=avx2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 8\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 01:33:49 deepsparse.transformers.utils.helpers INFO     Overwriting in-place the input shapes of the transformer model at /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 8\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    }
   ],
   "source": [
    "ds_model = DeepSparseCausalLM(\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    model_path=onnx_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "442c3dfd-c03e-4791-a1ae-212a2820857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish the following function for computing a fibonacci sequence: \n",
      "\n",
      " fib(n):\n",
      "\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Call the function.\n",
      "print(fib(5))\n",
      "\n",
      "# This code\n",
      "Finish the following function for computing a fibonacci sequence: \n",
      "\n",
      " fib(n):\n",
      "\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Call the function.\n",
      "print(fib(5))\n",
      "\n",
      "# This code\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Finish the following function for computing a fibonacci sequence: \\n\\n fib(n):\"\n",
    "\n",
    "def make_n_requests(n=1):\n",
    "    requests = []\n",
    "    for i in range(n):\n",
    "        request = Request(\n",
    "            id=i,\n",
    "            prompt=sequence,\n",
    "        )\n",
    "        requests.append(request)\n",
    "    return requests\n",
    "\n",
    "batch_size = 2\n",
    "batch = Batch(\n",
    "    id=0,\n",
    "    requests = make_n_requests(n=batch_size),\n",
    ")\n",
    "\n",
    "ds_batch = DeepSparseCausalLMBatch.from_batch(\n",
    "    batch=batch,\n",
    "    tokenizer=tokenizer, \n",
    ")\n",
    "\n",
    "next_batch = ds_batch\n",
    "for _ in range(64):\n",
    "    # print(tokenizer.batch_decode(next_batch.input_ids_list[0]))\n",
    "    generation, next_batch = ds_model.generate_token(next_batch)\n",
    "\n",
    "for input_ids in next_batch.input_ids_list:\n",
    "    print(tokenizer.batch_decode(input_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ba351-0e14-4440-9962-bb692599ae2a",
   "metadata": {},
   "source": [
    "## Compare to DeepSparse Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fc45233a-9a34-42bb-b6b0-7b19dd5763e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish the following function for computing a fibonacci sequence: \n",
      "\n",
      " fib(n):\n",
      "\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Call the function.\n",
      "print(fib(5))\n",
      "\n",
      "# This code is\n"
     ]
    }
   ],
   "source": [
    "multitoken_length = 4\n",
    "\n",
    "def sample_token(logits):\n",
    "    assert(logits.shape[0] == 1)        # assert b=1 for now\n",
    "    return np.argmax(logits[0,-1,:])  \n",
    "    \n",
    "def prefill_pipeline(pipeline, tokens):\n",
    "    num_tokens_processed = 0\n",
    "    for engine_inputs in pipeline.engine_inputs_for_prefill(tokens):\n",
    "        _, logits = pipeline.multitoken_engine(engine_inputs)\n",
    "        num_tokens_processed += multitoken_length\n",
    "    \n",
    "    if num_tokens_processed > 0:\n",
    "        pipeline.engine.transfer_cache_state(cache=pipeline.multitoken_engine.kv_cache)\n",
    "\n",
    "    run_tokens = [] if num_tokens_processed == 0 else tokens[:num_tokens_processed]\n",
    "    for token in tokens[num_tokens_processed:]:\n",
    "        run_tokens.append(token)\n",
    "        new_token, logits = pipeline.autoregressive_inference(run_tokens)\n",
    "    return logits\n",
    "    \n",
    "pipeline._reset_engines_cache()\n",
    "engine_inputs = pipeline.process_inputs(pipeline.parse_inputs(sequences=sequence))[0]\n",
    "tokens = engine_inputs[0][engine_inputs[1].nonzero()].tolist()\n",
    "\n",
    "logits = prefill_pipeline(pipeline, tokens)\n",
    "# print(logits)\n",
    "tokens.append(sample_token(logits))\n",
    "\n",
    "for _ in range(64):\n",
    "    _, logits = pipeline.autoregressive_inference(tokens)\n",
    "    # print(logits)\n",
    "    tokens.append(sample_token(logits))\n",
    "\n",
    "print(pipeline.tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac484d6-093d-411f-909a-2ac143b26cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsparse import Pipeline\n",
    "pipeline = Pipeline.create(\n",
    "    task=\"text-generation\", \n",
    "    model_path=\"zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none\",\n",
    "    use_deepsparse_cache=False,\n",
    "    prompt_processing_sequence_length=4,\n",
    "    max_generated_tokens=64,\n",
    "    sequence_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9574f0f7-c882-499a-ba8a-c107df0655ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 18)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_batch.input_ids_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eeb1449f-82f2-4bad-9265-5ddbf0944a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next_batch.input_ids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9a0104a8-3412-41a4-acd0-0dbbdf0fd9da",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dscb/lib/python3.9/site-packages/transformers/models/codegen/tokenization_codegen_fast.py:219\u001b[0m, in \u001b[0;36mCodeGenTokenizerFast.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, truncate_before_pattern, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    188\u001b[0m     token_ids: Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    193\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    tokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     decoded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m truncate_before_pattern \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(truncate_before_pattern) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    227\u001b[0m         decoded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate(decoded_text, truncate_before_pattern)\n",
      "File \u001b[0;32m~/.conda/envs/dscb/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3496\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3494\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dscb/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:549\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    548\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 549\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    552\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    555\u001b[0m )\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(next_batch.input_ids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce285999-6394-42b5-9c6b-d8e1743d068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "print(next_batch.input_ids_list[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d64cbf-e67d-4f24-b672-5365153a4781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "2023-08-21 18:14:09 deepsparse.transformers.utils.helpers INFO     Overwriting in-place the input shapes of the transformer model at /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.6.0.20230815 COMMUNITY | (134dba40) (release) (optimized) (system=avx2, binary=avx2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 8\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 18:14:33 deepsparse.transformers.utils.helpers INFO     Overwriting in-place the input shapes of the transformer model at /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsparse.engine.Engine:\n",
      "\tonnx_file_path: /home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\n",
      "\tbatch_size: 1\n",
      "\tnum_cores: 8\n",
      "\tnum_streams: 1\n",
      "\tscheduler: Scheduler.default\n",
      "\tfraction_of_supported_ops: 1.0\n",
      "\tcpu_avx_type: avx2\n",
      "\tcpu_vnni: False\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbb071c7-076a-469e-9cfe-a9b9e4108c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9]]\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([np.arange(10)]*2)\n",
    "b = np.array([np.arange(10)]*1)\n",
    "\n",
    "print(b[:,-1:])\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "53616cc6-ae91-410d-b6fa-4f0bd71be16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 18)\n",
      "(1, 19)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1fed0d-6930-4b03-96a1-04a7f6d13434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086c67f-a20a-44e8-865a-a026641d2761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74361be-2020-44e9-8646-0d14298e577d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6d438-ecd4-44a5-acd1-334c408a891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepsparse\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from server.text_generation_server.models.deepsparse_causal_lm import DeepSparseCausalLMBatch\n",
    "from text_generation_server.utils import NextTokenChooser, StoppingCriteria, Sampling, StopSequenceCriteria\n",
    "\n",
    "from server.text_generation_server.pb.generate_pb2 import (\n",
    "    Batch,    \n",
    "    Request, \n",
    "    NextTokenChooserParameters, \n",
    "    StoppingCriteriaParameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b86098-120f-4fff-9952-06a217494b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/deployment\"\n",
    "onnx_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ffcf7-a648-4a2c-a8b5-1eedc97ffa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_chooser = NextTokenChooser(\n",
    "    watermark=False,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=0.0,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    typical_p=None,\n",
    "    do_sample=False,\n",
    "    seed=0,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765bc684-d0cd-4c0d-bf52-33a90def89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_crtieria=StoppingCriteria(\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    stop_sequence_criterias=[],\n",
    "    max_new_tokens=20,\n",
    "    ignore_eos_token=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15489a78-44a0-412a-8a73-13b8552e6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Finish the following function for computing a fibonacci sequence: \\n\\n fib(n):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d015d8-d9fc-45a7-9d4a-c674c994084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_idx = 0\n",
    "\n",
    "max_new_tokens = 64\n",
    "\n",
    "parameters = NextTokenChooserParameters(\n",
    "    watermark=False,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=0.0,\n",
    "    do_sample=False,\n",
    "    typical_p=1.0,\n",
    "    top_k = 0,\n",
    "    top_p = 1.0,\n",
    ")\n",
    "\n",
    "stopping_parameters = StoppingCriteriaParameters(\n",
    "    max_new_tokens=max_new_tokens\n",
    ")\n",
    "\n",
    "def make_n_requests(n=1):\n",
    "    requests = []\n",
    "    for i in range(n):\n",
    "        request = Request(\n",
    "            id=request_idx,\n",
    "            inputs=sequence,\n",
    "            truncate=False,\n",
    "            parameters=parameters,\n",
    "            stopping_parameters=stopping_parameters,\n",
    "            prefill_logprobs=False\n",
    "        )\n",
    "        requests.append(request)\n",
    "    return requests\n",
    "\n",
    "batch_size = 2\n",
    "requests = make_n_requests(n=batch_size)\n",
    "\n",
    "batch = Batch(\n",
    "    id = 0,\n",
    "    requests = requests,\n",
    "    size=len(requests),\n",
    ")\n",
    "\n",
    "ds_batch = DeepSparseCausalLMBatch.from_pb(\n",
    "    pb=batch, \n",
    "    tokenizer=tokenizer, \n",
    "    dtype=torch.float32,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5873e4a-3c60-4764-9a78-85003bf4516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\n",
    "os.environ[\"WAND_OPT_FLAGS\"] = \"default,~pyramids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160e9fa-875b-4cb5-9284-d98fbda1c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from server.text_generation_server.models.deepsparse_model import DeepSparseDecoderModel, DeepSparsePastKeyValues\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/deployment\"\n",
    "onnx_path = \"/home/robertgshaw/.cache/sparsezoo/neuralmagic/codegen_mono-350m-bigpython_bigquery_thepile-base/model.onnx/model.onnx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f56f49-8dd9-4281-a37c-74011b4fdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_decoder_model = DeepSparseDecoderModel(\n",
    "    onnx_file_path = onnx_path,\n",
    "    sequence_length = 128,\n",
    "    multitoken_length = 4,\n",
    "    # singletoken_engine = ds_decoder_model.singletoken_engine,\n",
    "    # multitoken_engine = ds_decoder_model.multitoken_engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780b506-7a92-4b52-83a9-424d4337b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsparse import Pipeline\n",
    "pipeline = Pipeline.create(\n",
    "    task=\"text-generation\", \n",
    "    model_path=\"zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none\",\n",
    "    use_deepsparse_cache=False,\n",
    "    prompt_processing_sequence_length=4,\n",
    "    max_generated_tokens=64,\n",
    "    sequence_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abe7e2-98e4-4b5b-b2af-8c6037e71ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Finish the following function for computing a fibonacci sequence: \\n\\n fib(n):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff677bb4-e3dc-4201-bcb7-6b28da1cbf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample_token(logits):\n",
    "    assert(logits.shape[0] == 1)\n",
    "    return np.argmax(logits[0,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ead309-995b-4d96-9974-012be3fc46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"testing DeepSparseDecoderModel:\\n\")\n",
    "\n",
    "engine_inputs = pipeline.process_inputs(pipeline.parse_inputs(sequences=sequence))[0]\n",
    "tokens = engine_inputs[0][engine_inputs[1].nonzero()].tolist()\n",
    "\n",
    "past_key_values = DeepSparsePastKeyValues()\n",
    "logits, past_key_values = ds_decoder_model.prefill(tokens, past_key_values)\n",
    "tokens.append(sample_token(logits))\n",
    "\n",
    "while len(tokens) < 64:\n",
    "    logits, past_key_values = ds_decoder_model.decode(tokens, past_key_values)\n",
    "    tokens.append(sample_token(logits))\n",
    "\n",
    "print(pipeline.tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12819c0-0d74-4e68-9620-43f4ca9a69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "multitoken_length = 4\n",
    "\n",
    "def prefill_pipeline(pipeline, tokens):\n",
    "    num_tokens_processed = 0\n",
    "    for engine_inputs in pipeline.engine_inputs_for_prefill(tokens):\n",
    "        _, logits = pipeline.multitoken_engine(engine_inputs)\n",
    "        num_tokens_processed += multitoken_length\n",
    "\n",
    "    if num_tokens_processed > 0:\n",
    "        pipeline.engine.transfer_cache_state(cache=pipeline.multitoken_engine.kv_cache)\n",
    "\n",
    "    run_tokens = [] if num_tokens_processed == 0 else tokens[:num_tokens_processed]\n",
    "    for token in tokens[num_tokens_processed:]:\n",
    "        run_tokens.append(token)\n",
    "        new_token, logits = pipeline.autoregressive_inference(run_tokens)\n",
    "    return logits\n",
    "    \n",
    "pipeline._reset_engines_cache()\n",
    "engine_inputs = pipeline.process_inputs(pipeline.parse_inputs(sequences=sequence))[0]\n",
    "tokens = engine_inputs[0][engine_inputs[1].nonzero()].tolist()\n",
    "\n",
    "logits = prefill_pipeline(pipeline, tokens)\n",
    "tokens.append(sample_token(logits))\n",
    "\n",
    "while len(tokens) < 64:\n",
    "    _, logits = pipeline.autoregressive_inference(tokens)\n",
    "    tokens.append(sample_token(logits))\n",
    "\n",
    "print(pipeline.tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098f6f5-e745-4b08-be11-eaf8aa03f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sequence}{pipeline(sequences=sequence).sequences[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
